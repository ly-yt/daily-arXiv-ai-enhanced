{"id": "2508.14131", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14131", "abs": "https://arxiv.org/abs/2508.14131", "authors": ["Junjie Qi", "Siqi Mao", "Tianyi Tan"], "title": "An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents", "comment": null, "summary": "We propose an improved algorithm by identifying and encouraging cooperative\nbehavior in multi-agent environments. First, we analyze the shortcomings of\nexisting algorithms in addressing multi-agent reinforcement learning problems.\nThen, based on the existing algorithm MADDPG, we introduce a new parameter to\nincrease the reward that an agent can obtain when cooperative behavior among\nagents is identified. Finally, we compare our improved algorithm with MADDPG in\nenvironments from PettingZoo. The results show that the new algorithm helps\nagents achieve both higher team rewards and individual rewards."}
{"id": "2508.14214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14214", "abs": "https://arxiv.org/abs/2508.14214", "authors": ["Mattson Ogg", "Chace Ashcraft", "Ritwik Bose", "Raphael Norman-Tenazas", "Michael Wolmetz"], "title": "Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli", "comment": null, "summary": "Emotions exert an immense influence over human behavior and cognition in both\ncommonplace and high-stress tasks. Discussions of whether or how to integrate\nlarge language models (LLMs) into everyday life (e.g., acting as proxies for,\nor interacting with, human agents), should be informed by an understanding of\nhow these tools evaluate emotionally loaded stimuli or situations. A model's\nalignment with human behavior in these cases can inform the effectiveness of\nLLMs for certain roles or interactions. To help build this understanding, we\nelicited ratings from multiple popular LLMs for datasets of words and images\nthat were previously rated for their emotional content by humans. We found that\nwhen performing the same rating tasks, GPT-4o responded very similarly to human\nparticipants across modalities, stimuli and most rating scales (r = 0.9 or\nhigher in many cases). However, arousal ratings were less well aligned between\nhuman and LLM raters, while happiness ratings were most highly aligned. Overall\nLLMs aligned better within a five-category (happiness, anger, sadness, fear,\ndisgust) emotion framework than within a two-dimensional (arousal and valence)\norganization. Finally, LLM ratings were substantially more homogenous than\nhuman ratings. Together these results begin to describe how LLM agents\ninterpret emotional stimuli and highlight similarities and differences among\nbiological and artificial intelligence in key behavioral domains."}
{"id": "2508.14294", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14294", "abs": "https://arxiv.org/abs/2508.14294", "authors": ["Maria Leonor Pacheco", "Fabio Somenzi", "Dananjay Srinivas", "Ashutosh Trivedi"], "title": "Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions", "comment": null, "summary": "We propose a neurosymbolic approach to the explanation of complex sequences\nof decisions that combines the strengths of decision procedures and Large\nLanguage Models (LLMs). We demonstrate this approach by producing explanations\nfor the solutions of Hitori puzzles. The rules of Hitori include local\nconstraints that are effectively explained by short resolution proofs. However,\nthey also include a connectivity constraint that is more suitable for visual\nexplanations. Hence, Hitori provides an excellent testing ground for a flexible\ncombination of SAT solvers and LLMs. We have implemented a tool that assists\nhumans in solving Hitori puzzles, and we present experimental evidence of its\neffectiveness."}
{"id": "2508.14410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14410", "abs": "https://arxiv.org/abs/2508.14410", "authors": ["Beinuo Yang", "Qishen Zhou", "Junyi Li", "Xingchen Su", "Simon Hu"], "title": "Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning", "comment": null, "summary": "Optimization Modeling (OM) is essential for solving complex decision-making\nproblems. However, the process remains time-consuming and error-prone, heavily\nrelying on domain experts. While Large Language Models (LLMs) show promise in\naddressing these challenges through their natural language understanding and\nreasoning capabilities, current approaches face three critical limitations:\nhigh benchmark labeling error rates reaching up to 42\\%, narrow evaluation\nscope that only considers optimal values, and computational inefficiency due to\nheavy reliance on multi-agent systems or model fine-tuning. In this work, we\nfirst enhance existing datasets through systematic error correction and more\ncomprehensive annotation. Additionally, we introduce LogiOR, a new optimization\nmodeling benchmark from the logistics domain, containing more complex problems\nwith standardized annotations. Furthermore, we present ORThought, a novel\nframework that leverages expert-level optimization modeling principles through\nchain-of-thought reasoning to automate the OM process. Through extensive\nempirical evaluation, we demonstrate that ORThought outperforms existing\napproaches, including multi-agent frameworks, with particularly significant\nadvantages on complex optimization problems. Finally, we provide a systematic\nanalysis of our method, identifying critical success factors and failure modes,\nproviding valuable insights for future research on LLM-based optimization\nmodeling."}
{"id": "2508.14415", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14415", "abs": "https://arxiv.org/abs/2508.14415", "authors": ["Qiang Zhang", "Pei Yan", "Yijia Xu", "Chuanpo Fu", "Yong Fang", "Yang Liu"], "title": "The Agent Behavior: Model, Governance and Challenges in the AI Digital Age", "comment": null, "summary": "Advancements in AI have led to agents in networked environments increasingly\nmirroring human behavior, thereby blurring the boundary between artificial and\nhuman actors in specific contexts. This shift brings about significant\nchallenges in trust, responsibility, ethics, security and etc. The difficulty\nin supervising of agent behaviors may lead to issues such as data contamination\nand unclear accountability. To address these challenges, this paper proposes\nthe \"Network Behavior Lifecycle\" model, which divides network behavior into 6\nstages and systematically analyzes the behavioral differences between humans\nand agents at each stage. Based on these insights, the paper further introduces\nthe \"Agent for Agent (A4A)\" paradigm and the \"Human-Agent Behavioral Disparity\n(HABD)\" model, which examine the fundamental distinctions between human and\nagent behaviors across 5 dimensions: decision mechanism, execution efficiency,\nintention-behavior consistency, behavioral inertia, and irrational patterns.\nThe effectiveness of the model is verified through real-world cases such as red\nteam penetration and blue team defense. Finally, the paper discusses future\nresearch directions in dynamic cognitive governance architecture, behavioral\ndisparity quantification, and meta-governance protocol stacks, aiming to\nprovide a theoretical foundation and technical roadmap for secure and\ntrustworthy human-agent collaboration."}
{"id": "2508.14564", "categories": ["cs.AI", "cs.CL", "cs.HC", "I.2.9; I.2.10; I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2508.14564", "abs": "https://arxiv.org/abs/2508.14564", "authors": ["Luca Annese", "Sabrina Patania", "Silvia Serino", "Tom Foulsham", "Silvia Rossi", "Azzurra Ruggeri", "Dimitri Ognibene"], "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "comment": "Accepted at ICSR25", "summary": "Recent advances in large language models (LLMs) and reasoning frameworks have\nopened new possibilities for improving the perspective -taking capabilities of\nautonomous agents. However, tasks that involve active perception, collaborative\nreasoning, and perspective taking (understanding what another agent can see or\nknows) pose persistent challenges for current LLM-based systems. This study\ninvestigates the potential of structured examples derived from transformed\nsolution graphs generated by the Fast Downward planner to improve the\nperformance of LLM-based agents within a ReAct framework. We propose a\nstructured solution-processing pipeline that generates three distinct\ncategories of examples: optimal goal paths (G-type), informative node paths\n(E-type), and step-by-step optimal decision sequences contrasting alternative\nactions (L-type). These solutions are further converted into ``thought-action''\nexamples by prompting an LLM to explicitly articulate the reasoning behind each\ndecision. While L-type examples slightly reduce clarification requests and\noverall action steps, they do not yield consistent improvements. Agents are\nsuccessful in tasks requiring basic attentional filtering but struggle in\nscenarios that required mentalising about occluded spaces or weighing the costs\nof epistemic actions. These findings suggest that structured examples alone are\ninsufficient for robust perspective-taking, underscoring the need for explicit\nbelief tracking, cost modelling, and richer environments to enable socially\ngrounded collaboration in LLM-based agents."}
{"id": "2508.14644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14644", "abs": "https://arxiv.org/abs/2508.14644", "authors": ["Chendong Song", "Zihan Wang", "Frederick Pu", "Haiming Wang", "Xiaohan Lin", "Junqi Liu", "Jia Li", "Zhengying Liu"], "title": "LeanGeo: Formalizing Competitional Geometry problems in Lean", "comment": "28 pages", "summary": "Geometry problems are a crucial testbed for AI reasoning capabilities. Most\nexisting geometry solving systems cannot express problems within a unified\nframework, thus are difficult to integrate with other mathematical fields.\nBesides, since most geometric proofs rely on intuitive diagrams, verifying\ngeometry problems is particularly challenging. To address these gaps, we\nintroduce LeanGeo, a unified formal system for formalizing and solving\ncompetition-level geometry problems within the Lean 4 theorem prover. LeanGeo\nfeatures a comprehensive library of high-level geometric theorems with Lean's\nfoundational logic, enabling rigorous proof verification and seamless\nintegration with Mathlib. We also present LeanGeo-Bench, a formal geometry\nbenchmark in LeanGeo, comprising problems from the International Mathematical\nOlympiad (IMO) and other advanced sources. Our evaluation demonstrates the\ncapabilities and limitations of state-of-the-art Large Language Models on this\nbenchmark, highlighting the need for further advancements in automated\ngeometric reasoning. We open source the theorem library and the benchmark of\nLeanGeo at https://github.com/project-numina/LeanGeo/tree/master."}
{"id": "2508.14654", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14654", "abs": "https://arxiv.org/abs/2508.14654", "authors": ["Peilin Ji", "Xiao Xue", "Simeng Wang", "Wenhao Yan"], "title": "Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration", "comment": "17 pages including appendix, 6 figures", "summary": "In recent years, the increasing frequency of extreme urban rainfall events\nhas posed significant challenges to emergency scheduling systems. Urban\nflooding often leads to severe traffic congestion and service disruptions,\nthreatening public safety and mobility. However, effective decision making\nremains hindered by three key challenges: (1) managing trade-offs among\ncompeting goals (e.g., traffic flow, task completion, and risk mitigation)\nrequires dynamic, context-aware strategies; (2) rapidly evolving environmental\nconditions render static rules inadequate; and (3) LLM-generated strategies\nfrequently suffer from semantic instability and execution inconsistency.\nExisting methods fail to align perception, global optimization, and multi-agent\ncoordination within a unified framework. To tackle these challenges, we\nintroduce H-J, a hierarchical multi-agent framework that integrates\nknowledge-guided prompting, entropy-constrained generation, and feedback-driven\noptimization. The framework establishes a closed-loop pipeline spanning from\nmulti-source perception to strategic execution and continuous refinement. We\nevaluate H-J on real-world urban topology and rainfall data under three\nrepresentative conditions: extreme rainfall, intermittent bursts, and daily\nlight rain. Experiments show that H-J outperforms rule-based and\nreinforcement-learning baselines in traffic smoothness, task success rate, and\nsystem robustness. These findings highlight the promise of uncertainty-aware,\nknowledge-constrained LLM-based approaches for enhancing resilience in urban\nflood response."}
{"id": "2508.14704", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14704", "abs": "https://arxiv.org/abs/2508.14704", "authors": ["Ziyang Luo", "Zhiqi Shen", "Wenzhuo Yang", "Zirui Zhao", "Prathyusha Jwalapuram", "Amrita Saha", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Junnan Li"], "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "comment": "Website: https://mcp-universe.github.io", "summary": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem."}
{"id": "2508.14710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14710", "abs": "https://arxiv.org/abs/2508.14710", "authors": ["Swantje Plambeck", "Ali Salamati", "Eyke Huellermeier", "Goerschwin Fey"], "title": "Data-Driven Probabilistic Evaluation of Logic Properties with PAC-Confidence on Mealy Machines", "comment": null, "summary": "Cyber-Physical Systems (CPS) are complex systems that require powerful models\nfor tasks like verification, diagnosis, or debugging. Often, suitable models\nare not available and manual extraction is difficult. Data-driven approaches\nthen provide a solution to, e.g., diagnosis tasks and verification problems\nbased on data collected from the system. In this paper, we consider CPS with a\ndiscrete abstraction in the form of a Mealy machine. We propose a data-driven\napproach to determine the safety probability of the system on a finite horizon\nof n time steps. The approach is based on the Probably Approximately Correct\n(PAC) learning paradigm. Thus, we elaborate a connection between discrete logic\nand probabilistic reachability analysis of systems, especially providing an\nadditional confidence on the determined probability. The learning process\nfollows an active learning paradigm, where new learning data is sampled in a\nguided way after an initial learning set is collected. We validate the approach\nwith a case study on an automated lane-keeping system."}
{"id": "2508.14802", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14802", "abs": "https://arxiv.org/abs/2508.14802", "authors": ["Siyuan Song", "Harvey Lederman", "Jennifer Hu", "Kyle Mahowald"], "title": "Privileged Self-Access Matters for Introspection in AI", "comment": null, "summary": "Whether AI models can introspect is an increasingly important practical\nquestion. But there is no consensus on how introspection is to be defined.\nBeginning from a recently proposed ''lightweight'' definition, we argue instead\nfor a thicker one. According to our proposal, introspection in AI is any\nprocess which yields information about internal states through a process more\nreliable than one with equal or lower computational cost available to a third\nparty. Using experiments where LLMs reason about their internal temperature\nparameters, we show they can appear to have lightweight introspection while\nfailing to meaningfully introspect per our proposed definition."}
{"id": "2508.14131", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14131", "abs": "https://arxiv.org/abs/2508.14131", "authors": ["Junjie Qi", "Siqi Mao", "Tianyi Tan"], "title": "An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents", "comment": null, "summary": "We propose an improved algorithm by identifying and encouraging cooperative\nbehavior in multi-agent environments. First, we analyze the shortcomings of\nexisting algorithms in addressing multi-agent reinforcement learning problems.\nThen, based on the existing algorithm MADDPG, we introduce a new parameter to\nincrease the reward that an agent can obtain when cooperative behavior among\nagents is identified. Finally, we compare our improved algorithm with MADDPG in\nenvironments from PettingZoo. The results show that the new algorithm helps\nagents achieve both higher team rewards and individual rewards."}
